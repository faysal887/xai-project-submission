{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task Description"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal\n",
    "Explain the predictions of a graph neural network\n",
    "\n",
    "## Tasks\n",
    "1. Pick a dataset\n",
    "2. Analyze the dataset\n",
    "3. Train a graph neural network on the data and evaluate it\n",
    "4. Explain the graph neural network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "Suggested homogeneous datasets (one edge type)  \n",
    "▪ BA-Shapes: https://docs.dgl.ai/generated/dgl.data.BAShapeDataset.html  \n",
    "▪ BA-Community: https://docs.dgl.ai/generated/dgl.data.BACommunityDataset.html  \n",
    "▪ Tree-Cycles: https://docs.dgl.ai/generated/dgl.data.TreeCycleDataset.html  \n",
    "▪ Tree-Grid: https://docs.dgl.ai/generated/dgl.data.TreeGridDataset.html  \n",
    "▪ Cora: https://docs.dgl.ai/generated/dgl.data.CoraGraphDataset.html  \n",
    "\n",
    "Suggested heterogeneous datasets (multiple edge types)  \n",
    "▪ AIFB: https://data.dgl.ai/dataset/rdf/aifb-hetero.zip  \n",
    "▪ MUTAG: https://data.dgl.ai/dataset/rdf/am-hetero.zip  \n",
    "▪ BGS: https://data.dgl.ai/dataset/rdf/bgs-hetero.zip  \n",
    "▪ AM: https://data.dgl.ai/dataset/rdf/am-hetero.zip  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting graph data to tabular data  \n",
    "\n",
    "▪ Potential libraries to convert graph data to tabular data (not tested)  \n",
    "&emsp;&emsp;• https://derwen.ai/docs/kgl/ref/#build_df-method  \n",
    "&emsp;&emsp;• https://github.com/cadmiumkitty/rdfpandas  \n",
    "\n",
    "\n",
    "▪ Implement conversion yourself  \n",
    "&emsp;&emsp;• RDF library: https://rdflib.readthedocs.io/en/stable/intro_to_parsing.html  \n",
    "&emsp;&emsp;• “Table” library: https://pandas.pydata.org/docs/  \n",
    "&emsp;&emsp;• Given: set of triples: (subject, predicate, object)  \n",
    "&emsp;&emsp;• Each subject becomes a row in the table  \n",
    "&emsp;&emsp;• Each predicate becomes a column in the table  \n",
    "&emsp;&emsp;• Each object is inserted into a cell in the table  \n",
    "&emsp;&emsp;&emsp;&emsp;o If there are multiple objects for one subject and predicate, a cell contains multiple values  \n",
    "&emsp;&emsp;&emsp;&emsp;o If a subject does not have a certain predicate, the cell contains “NA” indicating a missing value  \n",
    "&emsp;&emsp;&emsp;&emsp;o The best way to deal with multiple values and NA values depends on dataset and use case  \n",
    "&emsp;&emsp;• Dealing with multiple values and NA values  \n",
    "&emsp;&emsp;&emsp;&emsp;o Simple approach: One-hot-encoding: for each combination of predicate and object, create a  \n",
    "&emsp;&emsp;&emsp;&emsp;column with a binary feature (0 or 1) indicating whether the subject is connected to the object via the predicate  \n",
    "&emsp;&emsp;• It is okay to convert only part of a dataset to a table (but try to make sure to achieve an appropriate fidelity,   \n",
    "&emsp;&emsp;&emsp;&emsp;i.e., the model trained on tabular data approximates the graph neural network well)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explaining the predictions\n",
    "\n",
    "Strategy 1: Explain surrogate model on tabular data  \n",
    "▪ Train a surrogate model that works on tabular data and approximates the graph neural network  \n",
    "    &emsp;&emsp;• Convert graph data to tabular data  \n",
    "    &emsp;&emsp;• Compute fidelity: measure how well the surrogate model approximates the graph neural network  \n",
    "\n",
    "▪ If surrogate model is interpretable: it can be explained directly  \n",
    "    &emsp;&emsp;• Weights of logistic regression  \n",
    "    &emsp;&emsp;• Decision Tree  \n",
    "\n",
    "▪ If surrogate model is not interpretable: it can be explained with an explainer  \n",
    "• Example models  \n",
    "    &emsp;&emsp;o random forest  \n",
    "    &emsp;&emsp;o neural network on tabular data  \n",
    "• Example explainers  \n",
    "    &emsp;&emsp;o LIME  \n",
    "    &emsp;&emsp;o ANCHOR  \n",
    "    &emsp;&emsp;o SHAP  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.datasets import Planetoid, TUDataset, KarateClub, GNNBenchmarkDataset, CitationFull\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import pickle\n",
    "from torch_geometric.explain.algorithm import GNNExplainer\n",
    "from torch_geometric.explain import Explainer\n",
    "\n",
    "pd.set_option('display.max_rows', 50000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Planetoid(root='data/Cora', name='Cora')\n",
    "\n",
    "data = dataset[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GNN, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, p):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=p, training=self.training) # p=0.2\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def train(p):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    # getting the indices for node features and egdes\n",
    "    x = dataset[0].x\n",
    "    edge_index = dataset[0].edge_index\n",
    "    # getting the labels for node§\n",
    "    y = dataset[0].y\n",
    "\n",
    "    try:\n",
    "        # training mask\n",
    "        train_mask = dataset[0].train_mask\n",
    "    except:\n",
    "        train_mask, val_mask, test_mask = get_mask(len(y))\n",
    "\n",
    "    # forward pass for gnn training\n",
    "    out = model(x, edge_index, p)\n",
    "    # training nodes loss\n",
    "    loss = F.nll_loss(out[train_mask], y[train_mask])\n",
    "    # backward pass for gnn training\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "def get_mask(i):\n",
    "\n",
    "    trn_len  = int(i*0.60)\n",
    "    val_len  = int(i*0.20)\n",
    "    test_len = int(i*0.20)\n",
    "\n",
    "    train_mask=  torch.tensor(([True]  * trn_len) + ([False] * val_len) + ([False] * test_len))\n",
    "    val_mask  =  torch.tensor(([False] * trn_len) + ([True]  * val_len) + ([False] * test_len))\n",
    "    test_mask =  torch.tensor(([False] * trn_len) + ([False] * val_len) + ([True]  * test_len))\n",
    "\n",
    "    return train_mask, val_mask, test_mask\n",
    "    \n",
    "\n",
    "def save_pickle(data, fname):\n",
    "    with open(fname, 'wb') as handle:\n",
    "        pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    return True\n",
    "\n",
    "\n",
    "def load_pickle(fname):\n",
    "    with open(fname, 'rb') as handle:\n",
    "        data = pickle.load(handle)\n",
    "        return data\n",
    "    \n",
    "@torch.no_grad()\n",
    "def test(p):\n",
    "    model.eval()\n",
    "    x = dataset[0].x\n",
    "    edge_index = dataset[0].edge_index\n",
    "    y = dataset[0].y\n",
    "    \n",
    "    try:\n",
    "        val_mask = dataset[0].val_mask\n",
    "        test_mask = dataset[0].test_mask\n",
    "        # print(val_mask)\n",
    "    except:\n",
    "        # val_mask, test_mask not present\n",
    "        train_mask, val_mask, test_mask = get_mask(len(y))\n",
    "\n",
    "\n",
    "    out = model(x, edge_index, p)\n",
    "    pred = out.argmax(dim=1)\n",
    "    # print('>> ', pred)\n",
    "    val_acc = pred[val_mask].eq(y[val_mask]).sum().item() / val_mask.sum().item()\n",
    "    test_acc = pred[test_mask].eq(y[test_mask]).sum().item() / test_mask.sum().item()\n",
    "    return val_acc, test_acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=10\n",
    "\n",
    "ps  = [0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9]\n",
    "lrs = [0.01, 0.015, 0.02, 0.025, 0.03, 0.035, 0.04, 0.45, 0.05, 0.055, 0.06, 0.065, 0.07, 0.075, 0.08, 0.085, 0.09, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "finaldf=pd.DataFrame()\n",
    "\n",
    "for i, p in enumerate(ps):\n",
    "    print(f'{i+1}/{len(ps)}', end='\\r')\n",
    "    \n",
    "    for lr in lrs:\n",
    "\n",
    "        # Create the model and optimizer\n",
    "        model = GNN(dataset.num_node_features, 16, dataset.num_classes)\n",
    "        # optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "        val_accs, test_accs=[], []\n",
    "        # Run for 200 epochs\n",
    "        for epoch in range(1, EPOCHS):\n",
    "            loss = train(p)\n",
    "            val_acc, test_acc = test(p)\n",
    "            val_accs.append(val_acc)\n",
    "            test_accs.append(test_acc)\n",
    "\n",
    "            # print(f'Epoch: {epoch}, Loss: {loss:.4f}, Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "\n",
    "        # print(max(val_accs), max(test_accs))\n",
    "\n",
    "        tmpdf=pd.DataFrame({\n",
    "                                'p':[p], \n",
    "                                'lr': [lr], \n",
    "                                'val_accs': [max(val_accs)], \n",
    "                                'test_accs': [max(test_accs)], \n",
    "                                'val_accs_iter': [val_accs.index(max(val_accs))], \n",
    "                                'test_accs_iter': [test_accs.index(max(test_accs))], \n",
    "                            })\n",
    "\n",
    "        finaldf = pd.concat([finaldf, tmpdf])\n",
    "\n",
    "        # torch.save(model, f'./models/p-{p}_lr-{lr}_val_accs-{max(val_accs)}_test_accs-{max(test_accs)}_{dataset_name}')\n",
    "        # save_pickle(model,  f'./models/p-{p}_lr-{lr}_val_accs-{max(val_accs)}_test_accs-{max(test_accs)}_{dataset_name}')\n",
    "\n",
    "finaldf.reset_index(drop=True).sort_values('test_accs', ascending=False).head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "li=os.listdir('./models/')\n",
    "resultsdf=pd.DataFrame()\n",
    "for x in li:\n",
    "    if 'val_accs' in x:\n",
    "        items=x.split('_')\n",
    "        tmpdf=pd.DataFrame([items], columns=['p','lr','val','val_acc','test','test_acc','data1','data2'])\n",
    "        resultsdf = pd.concat([resultsdf, tmpdf])\n",
    "\n",
    "resultsdf.sort_values('test_acc', ascending=False).reset_index(drop=True).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winner_model='_'.join(resultsdf.iloc[0].tolist())\n",
    "print(winner_model)\n",
    "\n",
    "shutil.copyfile(f'./models/{winner_model}', f'./best_model/{winner_model}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EdgeDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.x # a tensor of shape [2708, 1433], where 2708 is the number of nodes and 1433 is the number of features\n",
    "y = data.y # a tensor of shape [2708], where each element is an integer representing the class label of the node\n",
    "edge_index = data.edge_index # a tensor of shape [2, 10556], where each column is a pair of node indices representing an edge\n",
    "\n",
    "# converting the node features and labels to np arrays\n",
    "x = x.numpy()\n",
    "y = y.numpy()\n",
    "edge_index = edge_index.numpy()\n",
    "\n",
    "# create a list of triples from the edge index\n",
    "triples = []\n",
    "for i in range(edge_index.shape[1]):\n",
    "  # getting the source and target node indices\n",
    "  source = edge_index[0,i]\n",
    "  target = edge_index[1,i]\n",
    "  # creating a triple with the predicate 'cites'\n",
    "  triple = (source, 'cites', target)\n",
    "  # append the triple to the list\n",
    "  triples.append(triple)\n",
    "\n",
    "# creating a list of triples from the node features and labels\n",
    "for i in range(x.shape[0]):\n",
    "  node = i\n",
    "  features = x[i,:]\n",
    "  label = y[i]\n",
    "  # creating a triple with the predicate 'rdf:type' and the object 'Paper'\n",
    "  triple = (node, 'rdf:type', 'Paper')\n",
    "  # appending the triple to the list\n",
    "  triples.append(triple)\n",
    "  # creating a triple with the predicate 'label' and the object as the label value\n",
    "  triple = (node, 'label', label)\n",
    "  # appending the triple to the list\n",
    "  triples.append(triple)\n",
    "  # looping over the features and create triples with the predicate as 'word_i' and the object as the feature value\n",
    "  for j in range(features.shape[0]):\n",
    "    feature = features[j]\n",
    "    predicate = f'word_{j}'\n",
    "    triple = (node, predicate, feature)\n",
    "    # appending the triple to the list\n",
    "    triples.append(triple)\n",
    "\n",
    "edge_df = pd.DataFrame(triples, columns=['subject', 'predicate', 'object'])\n",
    "\n",
    "# Print the first five rows of the dataframe\n",
    "print(edge_df.shape)\n",
    "edge_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_df.subject.value_counts().to_frame().reset_index(drop=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_df.predicate.value_counts().to_frame().reset_index(drop=False).head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_df.object.value_counts().to_frame().reset_index(drop=False).head()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NodeDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "\n",
    "# Get the node features, labels and edge indices\n",
    "x = data.x # a tensor of shape [2708, 1433], where 2708 is the number of nodes and 1433 is the number of features\n",
    "y = data.y # a tensor of shape [2708], where each element is an integer representing the class label of the node\n",
    "edge_index = data.edge_index # a tensor of shape [2, 10556], where each column is a pair of node indices representing an edge\n",
    "\n",
    "# Convert the node features and labels to numpy arrays\n",
    "x = x.numpy()\n",
    "y = y.numpy()\n",
    "\n",
    "# Create a pandas dataframe from the node features and labels\n",
    "node_df = pd.DataFrame(x)\n",
    "node_df['label'] = y\n",
    "\n",
    "# Print the first five rows of the dataframe\n",
    "print(node_df.shape)\n",
    "node_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_df.describe()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(model)\n",
    "except:\n",
    "    model=load_pickle(os.listdir('./best_model/')[0])\n",
    "\n",
    "# create an Explainer object using the trained GNN model and the GNNExplainer algorithm\n",
    "explainer = Explainer(\n",
    "    model=model,\n",
    "    algorithm=GNNExplainer(epochs=20),\n",
    "    explanation_type=\"model\",\n",
    "    node_mask_type=\"attributes\",\n",
    "    edge_mask_type=\"object\",\n",
    "    model_config=dict(\n",
    "        mode=\"multiclass_classification\",\n",
    "        task_level=\"node\",\n",
    "        return_type=\"log_probs\", # Model returns log probabilities\n",
    "    ),\n",
    ")\n",
    "\n",
    "# explain the node prediction for the node at index 0 using 5 features\n",
    "explanation = explainer(data.x, data.edge_index, p=0.50, index=152)#, num_features=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation.node_attrs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation.get_explanation_subgraph()\n",
    "\n",
    "# node_mask=[43, 2879], : 43 nodes, 2879 features\n",
    "# edge_mask=[59],       : 59 edges\n",
    "# prediction=[43, 7], \n",
    "# target=[43], \n",
    "# index=23,             : node index\n",
    "# x=[43, 2879],         : 43 nodes, 2879 features\n",
    "# edge_index=[2, 59],   : \n",
    "# p=0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation.visualize_graph()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation.visualize_feature_importance(top_k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xai2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
